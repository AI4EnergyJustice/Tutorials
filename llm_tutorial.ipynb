{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cafda9cf",
   "metadata": {},
   "source": [
    "\n",
    "# Large Language Models Tutorial\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Large Language Models](#llms)\n",
    "2. [Using LLMs Programmatically](#api)\n",
    "3. [HuggingFace's Transformers Library](#huggingface)\n",
    "4. [Using LLMs with the Transformers Library](#example)\n",
    "\n",
    "---\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0e04d2",
   "metadata": {},
   "source": [
    "## 1. Large Language Models <a name=\"llms\"></a>\n",
    "\n",
    "Large Language Models (LLMs) are artificial intelligence models that\n",
    "have been trained to understand and generate text using human languages. They are based on a\n",
    "type of neural network architecture called transformers, which enables them to\n",
    "process and analyze large amounts of text data. LLMs learn to predict the next\n",
    "word in a sentence by being trained on large amounts of text data, which allows\n",
    "them to generate coherent and contextually relevant sentences. They have been\n",
    "used in a variety of applications, including question answering, text\n",
    "generation, translation, and summarization. What makes them particularly impressive is that they're not specifically programmed for each of these tasks. Instead, they learn patterns and structures from the data they are trained on and apply this knowledge to generate responses. These models are a great example of the power of Machine Learning, demonstrating how machines can learn from data to perform tasks that traditionally require human intelligence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb0c2f",
   "metadata": {},
   "source": [
    "## 2. Accessing LLMs Programmatically <a name=\"api\"></a>\n",
    "\n",
    "Accessing Large Language Models (LLMs) programmatically, as opposed to using a web interface, opens up a vast array of possibilities for their usage and integration into applications. By using a programming interface, you can directly interact with the model in a more flexible and customized manner. This is particularly useful in scenarios where you need to process large amounts of data, automate tasks, or integrate the model into a larger system. For example, you might want to use an LLM to analyze and generate responses to user queries on a website, generate content for a blog, or conduct large-scale analysis of text data. In a Jupyter notebook, you can experiment with the model, fine-tune its parameters, and visualize its outputs in a flexible, interactive environment. Furthermore, by accessing LLMs programmatically, you can leverage the full power and functionality of programming languages like Python, which offer a wide range of libraries and tools for data analysis, machine learning, and more. This approach enables more advanced usage of LLMs, and is a key skill for anyone working in the field of AI and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7afe5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "def chatGPT(prompt, model=\"text-davinci-003\"):\n",
    "    \"\"\"\n",
    "    Generate text using OpenAI's models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt : str\n",
    "        The initial text to base the AI's response on.\n",
    "    model : str, optional\n",
    "        The specific OpenAI model to use for the text generation.\n",
    "        Defaults to 'text-davinci-003'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : str\n",
    "        The function returns the AI-generated text and the  time taken to get the response\n",
    "    \"\"\"\n",
    "\n",
    "    # The endpoint for OpenAI's GPT-3 model\n",
    "    url = \"https://api.openai.com/v1/completions\"\n",
    "\n",
    "    # Retrieve the API key from environment variables\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        api_key = input(\"OPENAI_API_KEY is not defined, please enter it: \")\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "    # Define the headers for the HTTP request\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + api_key,\n",
    "    }\n",
    "\n",
    "    # Define the data for the HTTP request\n",
    "    data = {\n",
    "        \"model\": model,  # Specify the model to use\n",
    "        \"prompt\": prompt,  # The initial text to base the response on\n",
    "        \"max_tokens\": 2000,  # The maximum length of the generated text\n",
    "        \"temperature\": 1.0,  # The randomness of the generated text\n",
    "    }\n",
    "\n",
    "    # Record the time before sending the request\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Send a POST request to the OpenAI API\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    # Record the time after receiving the response\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the elapsed time (in seconds)\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Extract the generated text from the response\n",
    "    if response.status_code == 200:\n",
    "        # If the status code is 200, the request was successful\n",
    "        output = response.json()[\"choices\"][0][\"text\"]\n",
    "    else:\n",
    "        # If the status code is not 200, something went wrong\n",
    "        output = f\"Error: {response.json().get('error', 'Unknown error')}\"\n",
    "\n",
    "    elapsed_str = f\"\\nTime to get the response: {elapsed_time:.3f} seconds\"\n",
    "    return output + '\\n' + '-'*50 + elapsed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9067bfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Energy justice is an approach to energy that seeks to ensure equitable access to energy services, protect natural resources, and promote public participation in energy decision-making. It also focuses on reducing disparities in energy affordability, access, and usage for historically underserved communities and those vulnerable to impacts from the energy system.\n",
      "--------------------------------------------------\n",
      " Time taken to get the response: 4.276 seconds\n"
     ]
    }
   ],
   "source": [
    "output = chatGPT(\"What does energy justice mean?\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5717e1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dfce79963543bdb10a4af98b90c661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Prompt:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64439ae7a12942479eefd8ea8cd49650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options=('text-davinci-003', 'text-davinci-002', 'text-davinci-001', 'text-curi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ca01fd6fde4f47a5ae86cd5ad61559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Text', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb7e2f3cc14459aa950a8e4d6c750bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "# Create widgets\n",
    "prompt_widget = widgets.Textarea(value='',\n",
    "                                 placeholder='Type something',\n",
    "                                 description='Prompt:',\n",
    "                                 disable=False)\n",
    "\n",
    "# See https://platform.openai.com/docs/models/overview\n",
    "model_widget = widgets.Dropdown(options=['text-davinci-003', 'text-davinci-002', 'text-davinci-001', 'text-curie-001', 'text-babbage-001', 'text-ada-001', 'davinci', 'curie', 'babbage', 'ada'],\n",
    "                                value='text-davinci-003',\n",
    "                                description='Model:',\n",
    "                                disable=False)\n",
    "\n",
    "output_widget = widgets.Output(layout={'border': '1px solid black'})\n",
    "output_widget.append_stdout('')\n",
    "# Function to be executed on button click\n",
    "def on_button_clicked(b):\n",
    "    output = chatGPT(prompt_widget.value, model_widget.value)\n",
    "    with output_widget:\n",
    "        output_widget.clear_output()  # clear the output area each time the button is clicked\n",
    "        print(output)\n",
    "\n",
    "# Create a button\n",
    "button = widgets.Button(description=\"Generate Text\")\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "display(prompt_widget, model_widget, button, output_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4015ecf",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Transformers <a name=\"transformers\"></a>\n",
    "\n",
    "Transformers are a type of model architecture used in the field of Machine Learning, particularly for tasks involving natural language processing (NLP). Introduced in the seminal paper \"Attention is All You Need\" by Vaswani et al., the Transformer model revolutionized NLP by effectively dealing with the challenges of understanding the context and meaning in language data. The key innovation of Transformers is the mechanism called \"attention,\" which allows the model to weigh the importance of different words in a sentence when generating a prediction. Unlike previous methods that processed words in a sequence one after another, Transformers can process all words in a sentence simultaneously, making them more efficient and capable of capturing complex patterns in language. \n",
    "\n",
    "LLMs like GPT-3 and BERT are based on the transformer\n",
    "architecture. These models have many millions, or even billions, of parameters\n",
    "that are trained on large amounts of text data and they have set new standards in tasks like text generation, translation, and question answering. \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6ae9658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (4.30.2)\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.20.tar.gz (7.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from transformers) (1.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.12 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from xformers) (2.0.1)\n",
      "Collecting pyre-extensions==0.0.29\n",
      "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-inspect in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from pyre-extensions==0.0.29->xformers) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from pyre-extensions==0.0.29->xformers) (4.7.1)\n",
      "Requirement already satisfied: fsspec in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: sympy in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from torch>=1.12->xformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from torch>=1.12->xformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from torch>=1.12->xformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from jinja2->torch>=1.12->xformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from sympy->torch>=1.12->xformers) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from typing-inspect->pyre-extensions==0.0.29->xformers) (1.0.0)\n",
      "Installing collected packages: pyre-extensions, xformers\n",
      "\u001b[33m  DEPRECATION: xformers is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for xformers ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed pyre-extensions-0.0.29 xformers-0.0.20\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install transformers library\n",
    "%pip install transformers xformers\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e02c1",
   "metadata": {},
   "source": [
    "\n",
    "Remember to import the necessary libraries at the start of your notebook:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e723bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of transformers: 4.30.2\n",
      "Version of xformers: 0.0.20\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import xformers\n",
    "print(f\"Version of transformers: {transformers.__version__}\")\n",
    "print(f\"Version of xformers: {xformers.__version__}\")\n",
    "\n",
    "from transformers import pipeline\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acde912",
   "metadata": {},
   "source": [
    "\n",
    "## 3. HuggingFace's Transformers Library <a name=\"huggingface\"></a>\n",
    "\n",
    "From Transformers [website](https://huggingface.co/transformers):\n",
    "\n",
    "HuggingFace Transformers is a Python library that provides state-of-the-art pre-trained models for natural language processing (NLP) and natural language generation (NLG). The library includes models for a wide range of tasks, such as text classification, question answering, summarization, translation, and text generation.\n",
    "\n",
    "Transformers is built on top of PyTorch and TensorFlow, and it provides APIs that make it easy to download, load, and use pre-trained models. The library also includes a number of tools for fine-tuning pre-trained models for specific tasks, and for evaluating the performance of models.\n",
    "\n",
    "HuggingFace Transformers is a popular library for NLP and NLG research. It is used by a wide range of researchers and developers, and it is supported by a large community of contributors.\n",
    "\n",
    "Here are some of the features of HuggingFace Transformers:\n",
    "\n",
    "* A wide range of pre-trained models: Transformers includes models for a wide range of NLP and NLG tasks, in over 100 languages.\n",
    "* Easy to use APIs: The library provides APIs that make it easy to download, load, and use pre-trained models.\n",
    "* Tools for fine-tuning: Transformers includes tools for fine-tuning pre-trained models for specific tasks.\n",
    "* A large community: Transformers is a popular library with a large community of contributors.\n",
    "\n",
    "If you are interested in NLP or NLG research, then HuggingFace Transformers is a great library to use. It provides a wide range of pre-trained models, easy to use APIs, and tools for fine-tuning models.\n",
    "\n",
    "Here are some links to learn more about HuggingFace Transformers:\n",
    "\n",
    "* Documentation:        https://huggingface.co/docs/transformers/index\n",
    "* GitHub repository:    https://github.com/huggingface/transformers        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c906a45",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Using LLMs with the Transformers Library <a name=\"example\"></a>\n",
    "\n",
    "For an example project, we could demonstrate how to use the ChatGPT model with the Transformers library. Note that the next step involves downloading the model, so can take significant time with a slow connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2be3e50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of the USA is Denver and that is why the only real-world version of the Denver movie theater is really called \"The Colorado.\"\n",
      "\n",
      "The real Denver movie theater, at 614 Main St, is an American cinema, but is\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the GPT-2 model\n",
    "gpt2 = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Use the model to generate a response\n",
    "response = gpt2('The capital of the USA is', max_length=50)\n",
    "\n",
    "print(response[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf2295-1c79-4edd-86a8-1f2974f4fce3",
   "metadata": {},
   "source": [
    "As you can see above, LLMs can generate false response. The model likely made these mistakes because it doesn't actually understand geography or factual data about the world. Instead, it generates text based on patterns it learned during training. Thus, it's important to remember that while AI language models can produce impressively human-like text, they can also propagate misinformation and should always be used with an understanding of their limitations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "44e5a0b352bb81bb64b08912c126ba184b85b2b3fe908e7640b1e49d3d6685b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
