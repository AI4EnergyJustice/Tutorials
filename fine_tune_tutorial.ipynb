{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9624bd81",
   "metadata": {},
   "source": [
    "\n",
    "# Customizing Large Language Models with Additional Input\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Customizing Large Language Models](#introduction)\n",
    "2. [Question-Answering LLMs](#qa)\n",
    "3. [Setting up the Environment](#setup)\n",
    "4. [Paper-QA](#paper)\n",
    "5. [Demo](#demo)\n",
    "\n",
    "---\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c1275",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Customizing Large Language Models <a name=\"introduction\"></a>\n",
    "\n",
    " Customizing Large Language Models (LLMs) with additional data is a powerful way\n",
    " to tailor their capabilities to specific tasks or domains. This process, often\n",
    " referred to as \"fine-tuning,\" involves training the model on a new dataset that\n",
    " is related to the specific task at hand. The new data effectively guides the\n",
    " model to adjust its internal parameters and better align its language\n",
    " generation capabilities with the desired task. For instance, you might\n",
    " fine-tune a general-purpose language model on medical literature to create a\n",
    " model that excels at answering medical questions. Or you could fine-tune a\n",
    " model on customer support transcripts to create a chatbot that understands the\n",
    " specific language and issues related to a particular product or service.\n",
    " Fine-tuning allows us to leverage the power of LLMs that have been trained on\n",
    " vast amounts of data, while still creating models that are highly specialized\n",
    " and effective in specific domains or tasks.\n",
    " \n",
    " ## 2. Question-Answering LLMs <a name=\"qa\"></a>\n",
    "\n",
    "Question-Answering (QA) Large Language Models are a specialized application of LLMs that have been fine-tuned to answer questions based on provided context or broad knowledge learned during training. These models can interpret a wide range of questions and provide precise answers, making them extremely useful in applications like chatbots, virtual assistants, and customer service automation. Some QA models are designed to generate answers based on a specific piece of text or a set of documents, while others can answer questions based on a broad range of general knowledge. The latter, known as \"open-domain\" QA models, can answer questions about virtually any topic, drawing on the vast amounts of information they were trained on. Examples of open-domain QA models include GPT-3 by OpenAI and T5 by Google. These models have significantly advanced the field of natural language understanding and opened up new possibilities for AI-powered question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd33d3e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Setting up the Environment <a name=\"setup\"></a>\n",
    "\n",
    "Before we start coding, we need to install the necessary libraries. This can be done by running the following commands in your Jupyter notebook:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a3b69d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: paper-qa in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (3.5.0)\n",
      "Requirement already satisfied: pypdf in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (3.15.0)\n",
      "Requirement already satisfied: langchain>=0.0.198 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (0.0.255)\n",
      "Requirement already satisfied: openai>=0.27.8 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (0.27.8)\n",
      "Requirement already satisfied: faiss-cpu in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (1.7.4)\n",
      "Requirement already satisfied: PyCryptodome in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (3.18.0)\n",
      "Requirement already satisfied: html2text in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (2020.1.16)\n",
      "Requirement already satisfied: tiktoken>=0.4.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (0.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (2.0.19)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (0.0.19)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (2.8.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (1.24.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (8.2.2)\n",
      "Requirement already satisfied: tqdm in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from openai>=0.27.8->paper-qa) (4.65.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from tiktoken>=0.4.0->paper-qa) (2023.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from pydantic<2,>=1->langchain>=0.0.198->paper-qa) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.198->paper-qa) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install paper-qa        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990fb7b8-0f64-4b9e-8c1f-324ca2fce2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    api_key = input(\"OPENAI_API_KEY is not defined, please enter it: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04fae96b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaperQA version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import paperqa\n",
    "print('PaperQA version:', paperqa.__version__)\n",
    "\n",
    "# Required for Jupyter Notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da7279",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Paper QA <a name=\"paper\"></a>\n",
    "\n",
    "Paper QA is a minimal package for doing question and answering from\n",
    "PDFs, HTML or raw text files. It aims to give very good answers, with no hallucinations, by grounding responses with in-text citations.\n",
    "\n",
    "By default, it uses [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings) with a vector DB called [FAISS](https://github.com/facebookresearch/faiss) to embed and search documents. However, via [langchain](https://github.com/hwchase17/langchain) you can use open-source models or embeddings (see details below).\n",
    "\n",
    "PaperQA uses the process shown below:\n",
    "\n",
    "1. embed docs into vectors\n",
    "2. embed query into vector\n",
    "3. search for top k passages in docs\n",
    "4. create summary of each passage relevant to query\n",
    "5. put summaries into prompt\n",
    "6. generate answer with prompt\n",
    "\n",
    "## 5. Demo <a name=\"demo\"></a>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb66cd0",
   "metadata": {},
   "source": [
    "### Before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabf7fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Justice40 initiative?\n",
      "\n",
      "I cannot answer this question due to insufficient information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from paperqa import Docs\n",
    "\n",
    "# Pricing info for OpenAI models: https://openai.com/pricing#language-models\n",
    "docs = Docs(llm='gpt-4') # Better model, but more expensive\n",
    "docs = Docs(llm='gpt-3.5-turbo') # Faster and cheaper\n",
    "answer = docs.query(\"What is Justice40 initiative?\")\n",
    "print(answer.formatted_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948e2c5",
   "metadata": {},
   "source": [
    "### Add a document describing Justice40 initiative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ece691fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the document took 4.53 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "docs.add_url('https://www.whitehouse.gov/environmentaljustice/justice40/')\n",
    "end = time.time()\n",
    "print(f'Adding the document took {(end-start):.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a86644a",
   "metadata": {},
   "source": [
    "### After fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b53869-2aad-413f-a689-c0d91a1758f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Justice40 initiative?\n",
      "\n",
      "The Justice40 Initiative is a commitment made by the Federal Government to ensure that 40 percent of the overall benefits of certain Federal investments flow to disadvantaged communities that are marginalized, underserved, and overburdened by pollution. It covers a range of investments including climate change, clean energy and energy efficiency, clean transit, affordable and sustainable housing, training and workforce development, remediation and reduction of legacy pollution, and the development of critical clean water and wastewater infrastructure. The initiative is a continuous effort to improve how government programs deliver benefits to disadvantaged communities and aims to address underinvestment, environmental injustice, and the climate crisis (The2023 chunk 21, The2023 chunk 22).\n",
      "\n",
      "References\n",
      "\n",
      "1. (The2023 chunk 21): \"The White House.\" The White House, 2023, www.whitehouse.gov/environmentaljustice/justice40/.\n",
      "\n",
      "2. (The2023 chunk 22): \"The White House.\" The White House, 2023, www.whitehouse.gov/environmentaljustice/justice40/.\n",
      "\n",
      "Fine-tuning and query took 10.89 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "answer = docs.query(\"What is Justice40 initiative?\")\n",
    "end = time.time()\n",
    "print(answer.formatted_answer)\n",
    "print(f'Fine-tuning and query took {(end-start):.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84370005-5966-4471-881f-bfb2047530f4",
   "metadata": {},
   "source": [
    "## Running a local model\n",
    "\n",
    "You can run Paper-QA with any local model supported by Langchain. Please note that you can also use other tools for fine tuning such as https://www.llamaindex.ai/, https://github.com/imartinez/privateGPT, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "946c6e65-36ba-45b1-9d96-df5276ece308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-cpp-python in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (0.1.77)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from llama-cpp-python) (4.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from llama-cpp-python) (1.24.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from llama-cpp-python) (5.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gpt4all\n",
      "  Obtaining dependency information for gpt4all from https://files.pythonhosted.org/packages/32/74/542dbc9e58cc92b07bfb8c38f7a4d4d9057da5b50a8379dd16304c6d49ec/gpt4all-1.0.8-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading gpt4all-1.0.8-py3-none-manylinux1_x86_64.whl.metadata (912 bytes)\n",
      "Requirement already satisfied: requests in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from gpt4all) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from gpt4all) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from requests->gpt4all) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from requests->gpt4all) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from requests->gpt4all) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from requests->gpt4all) (2023.7.22)\n",
      "Downloading gpt4all-1.0.8-py3-none-manylinux1_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gpt4all\n",
      "Successfully installed gpt4all-1.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-cpp-python\n",
    "%pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3538f41-b84e-4b0e-bb8a-b7b8b674865e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q2_K.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 10 (mostly Q2_K)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3259.66 MB (+ 2048.00 MB per state)\n",
      "llama_model_load_internal: offloading 0 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 0/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 512 MB\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "llama.cpp: loading model from /global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q2_K.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 10 (mostly Q2_K)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3259.66 MB (+  512.00 MB per state)\n",
      "llama_model_load_internal: offloading 0 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 0/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 288 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Justice40 initiative?\n",
      "\n",
      "I cannot answer this question due to insufficient information.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: kv self size  =  512.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from paperqa import Docs\n",
    "from langchain.llms import LlamaCpp, GPT4All\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "model_path=\"/global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/alpaca-native-7B-ggml/ggml-model-q8_0.bin\"\n",
    "model_path=\"/global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q8_0.bin\"\n",
    "model_path=\"/global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q4_1.bin\"\n",
    "model_path=\"/global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q2_K.bin\"\n",
    "#model_path=\"/global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/ggml-gpt4all-l13b\"\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "# LlamaCpp model\n",
    "llm = LlamaCpp(model_path=model_path, callbacks=callbacks, n_ctx=4096)\n",
    "embeddings = LlamaCppEmbeddings(model_path=model_path)\n",
    "\n",
    "# GPT4All model\n",
    "# Verbose is required to pass to the callback manager\n",
    "# llm = GPT4All(model=model_path, callbacks=callbacks, verbose=True)\n",
    "# embeddings = GPT4AllEmbeddings(model_path=model_path)\n",
    "\n",
    "docs = Docs(llm=llm, embeddings=embeddings)\n",
    "answer = docs.query(\"What is Justice40 initiative?\")\n",
    "print(answer.formatted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ec576-297b-4c5c-932b-31fa1050319b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "docs.add_url('https://www.whitehouse.gov/environmentaljustice/justice40/')\n",
    "end = time.time()\n",
    "print(f'Adding the document took {(end-start):.2f} seconds')\n",
    "\n",
    "answer = docs.query(\"What is Justice40 initiative?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0acb9182-3359-4ffe-926f-b1ddbbaa9e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/ggml-gpt4all-l13b/ggml-gpt4all-l13b-snoozy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/ggml-gpt4all-l13b/ggml-gpt4all-l13b-snoozy.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  17.47 KB\n",
      "llama_model_load_internal: mem required  = 3976.51 MB (+ 1608.00 MB per state)\n",
      "error loading model: llama.cpp: tensor 'layers.9.attention_norm.weight' is missing from model\n",
      "llama_init_from_file: failed to load model\n",
      "LLAMA ERROR: failed to load model from /global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/ggml-gpt4all-l13b/ggml-gpt4all-l13b-snoozy.bin\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Model not loaded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [StreamingStdOutCallbackHandler()]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Verbose is required to pass to the callback manager\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# If you want to use a custom model add the backend parameter\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Check https://docs.gpt4all.io/gpt4all_python.html for supported backends\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# llm = GPT4All(model=local_path, backend=\"gptj\", callbacks=callbacks, verbose=True)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(prompt\u001b[38;5;241m=\u001b[39mprompt, llm\u001b[38;5;241m=\u001b[39mllm)\n",
      "File \u001b[0;32m/global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages/pydantic/main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages/langchain/llms/gpt4all.py:148\u001b[0m, in \u001b[0;36mGPT4All.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    140\u001b[0m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m GPT4AllModel(\n\u001b[1;32m    141\u001b[0m     model_name,\n\u001b[1;32m    142\u001b[0m     model_path\u001b[38;5;241m=\u001b[39mmodel_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    143\u001b[0m     model_type\u001b[38;5;241m=\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    144\u001b[0m     allow_download\u001b[38;5;241m=\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_download\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    145\u001b[0m )\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# set n_threads\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclient\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_thread_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_threads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmodel_type\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/gpt4all/pyllmodel.py:204\u001b[0m, in \u001b[0;36mLLModel.set_thread_count\u001b[0;34m(self, n_threads)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_thread_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_threads):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m llmodel\u001b[38;5;241m.\u001b[39mllmodel_isModelLoaded(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel):\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel not loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m     llmodel\u001b[38;5;241m.\u001b[39mllmodel_setThreadCount(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, n_threads)\n",
      "\u001b[0;31mException\u001b[0m: Model not loaded"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "local_path = \"/global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/ggml-gpt4all-l13b/ggml-gpt4all-l13b-snoozy.bin\"\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True, n_threads=6)\n",
    "\n",
    "# If you want to use a custom model add the backend parameter\n",
    "# Check https://docs.gpt4all.io/gpt4all_python.html for supported backends\n",
    "# llm = GPT4All(model=local_path, backend=\"gptj\", callbacks=callbacks, verbose=True)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fa0864a-7032-450f-b5e2-cfe317d82560",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: pip: command not found\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa8bae1b-da00-44f1-8ac7-587849abbccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=on\"\n",
    "os.environ[\"FORCE_CMAKE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "430777c2-7350-4a86-93ee-581364540841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.1.77.tar.gz (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
      "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Obtaining dependency information for numpy>=1.20.0 from https://files.pythonhosted.org/packages/32/6a/65dbc57a89078af9ff8bfcd4c0761a50172d90192eaeb1b6f56e5fbf1c3d/numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m322.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m369.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.77-cp311-cp311-linux_x86_64.whl size=1387471 sha256=a22865a5a72e9d3cf24184fd6681cdc3a3a29a6a40a4aead7865383bfd5df7cf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xi51btvx/wheels/e2/67/cb/481cfaabbb5fd5edab627c5b475de63e1b6f7d4d7b678d4d25\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.11 are installed in '/global/homes/k/keceli/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed diskcache-5.6.1 llama-cpp-python-0.1.77 numpy-1.25.2 typing-extensions-4.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e3e4a-830b-456b-91be-cd33431d10af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-bootcamp",
   "language": "python",
   "name": "hpc-bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "656fdc43c6a44647078f41991652796517ac498021f62375f4b7c9b76d0f8ed9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
