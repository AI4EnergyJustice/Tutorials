{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9624bd81",
   "metadata": {},
   "source": [
    "\n",
    "# Customizing Large Language Models with Additional Input\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Customizing Large Language Models](#introduction)\n",
    "2. [Question-Answering LLMs](#qa)\n",
    "3. [Setting up the Environment](#setup)\n",
    "4. [Paper-QA](#paper)\n",
    "5. [Demo](#demo)\n",
    "\n",
    "---\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c1275",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Customizing Large Language Models <a name=\"introduction\"></a>\n",
    "\n",
    " Customizing Large Language Models (LLMs) with additional data is a powerful way\n",
    " to tailor their capabilities to specific tasks or domains. This process, often\n",
    " referred to as \"fine-tuning,\" involves training the model on a new dataset that\n",
    " is related to the specific task at hand. The new data effectively guides the\n",
    " model to adjust its internal parameters and better align its language\n",
    " generation capabilities with the desired task. For instance, you might\n",
    " fine-tune a general-purpose language model on medical literature to create a\n",
    " model that excels at answering medical questions. Or you could fine-tune a\n",
    " model on customer support transcripts to create a chatbot that understands the\n",
    " specific language and issues related to a particular product or service.\n",
    " Fine-tuning allows us to leverage the power of LLMs that have been trained on\n",
    " vast amounts of data, while still creating models that are highly specialized\n",
    " and effective in specific domains or tasks.\n",
    " \n",
    " ## 2. Question-Answering LLMs <a name=\"qa\"></a>\n",
    "\n",
    "Question-Answering (QA) Large Language Models are a specialized application of LLMs that have been fine-tuned to answer questions based on provided context or broad knowledge learned during training. These models can interpret a wide range of questions and provide precise answers, making them extremely useful in applications like chatbots, virtual assistants, and customer service automation. Some QA models are designed to generate answers based on a specific piece of text or a set of documents, while others can answer questions based on a broad range of general knowledge. The latter, known as \"open-domain\" QA models, can answer questions about virtually any topic, drawing on the vast amounts of information they were trained on. Examples of open-domain QA models include GPT-3 by OpenAI and T5 by Google. These models have significantly advanced the field of natural language understanding and opened up new possibilities for AI-powered question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd33d3e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Setting up the Environment <a name=\"setup\"></a>\n",
    "\n",
    "Before we start coding, we need to install the necessary libraries. This can be done by running the following commands in your Jupyter notebook:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a3b69d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: paper-qa in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (3.5.0)\n",
      "Requirement already satisfied: pypdf in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (3.15.0)\n",
      "Requirement already satisfied: langchain>=0.0.198 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (0.0.255)\n",
      "Requirement already satisfied: openai>=0.27.8 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (0.27.8)\n",
      "Requirement already satisfied: faiss-cpu in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (1.7.4)\n",
      "Requirement already satisfied: PyCryptodome in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (3.18.0)\n",
      "Requirement already satisfied: html2text in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (2020.1.16)\n",
      "Requirement already satisfied: tiktoken>=0.4.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from paper-qa) (0.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (2.0.19)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (0.0.19)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (2.8.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (1.24.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from langchain>=0.0.198->paper-qa) (8.2.2)\n",
      "Requirement already satisfied: tqdm in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from openai>=0.27.8->paper-qa) (4.65.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from tiktoken>=0.4.0->paper-qa) (2023.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from pydantic<2,>=1->langchain>=0.0.198->paper-qa) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.198->paper-qa) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install paper-qa        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990fb7b8-0f64-4b9e-8c1f-324ca2fce2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not defined, please enter it:  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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    api_key = input(\"OPENAI_API_KEY is not defined, please enter it: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04fae96b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaperQA version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import paperqa\n",
    "print('PaperQA version:', paperqa.__version__)\n",
    "\n",
    "# Required for Jupyter Notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da7279",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Paper QA <a name=\"paper\"></a>\n",
    "\n",
    "Paper QA is a minimal package for doing question and answering from\n",
    "PDFs, HTML or raw text files. It aims to give very good answers, with no hallucinations, by grounding responses with in-text citations.\n",
    "\n",
    "By default, it uses [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings) with a vector DB called [FAISS](https://github.com/facebookresearch/faiss) to embed and search documents. However, via [langchain](https://github.com/hwchase17/langchain) you can use open-source models or embeddings (see details below).\n",
    "\n",
    "PaperQA uses the process shown below:\n",
    "\n",
    "1. embed docs into vectors\n",
    "2. embed query into vector\n",
    "3. search for top k passages in docs\n",
    "4. create summary of each passage relevant to query\n",
    "5. put summaries into prompt\n",
    "6. generate answer with prompt\n",
    "\n",
    "## 5. Demo <a name=\"demo\"></a>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb66cd0",
   "metadata": {},
   "source": [
    "### Before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabf7fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Justice40 initiative?\n",
      "\n",
      "I cannot answer this question due to insufficient information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from paperqa import Docs\n",
    "\n",
    "docs = Docs(llm='gpt-4')\n",
    "answer = docs.query(\"What is Justice40 initiative?\")\n",
    "print(answer.formatted_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948e2c5",
   "metadata": {},
   "source": [
    "### Add a document describing Justice40 initiative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece691fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the document took 17.89 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "docs.add_url('https://www.whitehouse.gov/environmentaljustice/justice40/')\n",
    "end = time.time()\n",
    "print(f'Adding the document took {(end-start):.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a86644a",
   "metadata": {},
   "source": [
    "### After fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b53869-2aad-413f-a689-c0d91a1758f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "answer = docs.query(\"What is Justice40 initiative?\")\n",
    "end = time.time()\n",
    "print(answer.formatted_answer)\n",
    "print(f'Fine-tuning and query took {(end-start):.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84370005-5966-4471-881f-bfb2047530f4",
   "metadata": {},
   "source": [
    "## Running a local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946c6e65-36ba-45b1-9d96-df5276ece308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (0.1.77)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from llama-cpp-python) (4.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from llama-cpp-python) (1.24.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages (from llama-cpp-python) (5.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3538f41-b84e-4b0e-bb8a-b7b8b674865e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 3 (mostly Q4_1)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4543.35 MB (+ 2048.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama.cpp: loading model from /global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 3 (mostly Q4_1)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4543.35 MB (+  512.00 MB per state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Justice40 initiative?\n",
      "\n",
      "I cannot answer this question due to insufficient information.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: kv self size  =  512.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from paperqa import Docs\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "modelpath=\"/global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/alpaca-native-7B-ggml/ggml-model-q8_0.bin\"\n",
    "modelpath=\"/global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q8_0.bin\"\n",
    "modelpath=\"/global/cfs/cdirs/m4388/Project1-AI4EJ/Tutorials/models/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q4_1.bin\"\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=ggml_q40, callbacks=[StreamingStdOutCallbackHandler()], n_ctx=4096\n",
    ")\n",
    "embeddings = LlamaCppEmbeddings(model_path=modelpath)\n",
    "docs = Docs(llm=llm, embeddings=embeddings)\n",
    "answer = docs.query(\"What is Justice40 initiative?\")\n",
    "print(answer.formatted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1ec576-297b-4c5c-932b-31fa1050319b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please provide the citation for the HTML code in MLA format for the text provided above. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   573.88 ms\n",
      "llama_print_timings:      sample time =    15.59 ms /    22 runs   (    0.71 ms per token,  1411.52 tokens per second)\n",
      "llama_print_timings: prompt eval time = 294232.23 ms /  3183 tokens (   92.44 ms per token,    10.82 tokens per second)\n",
      "llama_print_timings:        eval time = 10999.29 ms /    21 runs   (  523.78 ms per token,     1.91 tokens per second)\n",
      "llama_print_timings:       total time = 306135.76 ms\n",
      "llama_tokenize_with_model: too many tokens\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (8,) into shape (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdocs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://www.whitehouse.gov/environmentaljustice/justice40/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m answer \u001b[38;5;241m=\u001b[39m docs\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Justice40 initiative?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "File \u001b[0;32m/global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages/paperqa/docs.py:160\u001b[0m, in \u001b[0;36mDocs.add_url\u001b[0;34m(self, url, citation, docname, dockey, chunk_chars)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(url) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# need to wrap to enable seek\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     file \u001b[38;5;241m=\u001b[39m BytesIO(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcitation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcitation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdockey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdockey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_chars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages/paperqa/docs.py:138\u001b[0m, in \u001b[0;36mDocs.add_file\u001b[0;34m(self, file, citation, docname, dockey, chunk_chars)\u001b[0m\n\u001b[1;32m    136\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(file\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    137\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcitation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcitation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdockey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdockey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_chars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages/paperqa/docs.py:225\u001b[0m, in \u001b[0;36mDocs.add\u001b[0;34m(self, path, citation, docname, disable_check, dockey, chunk_chars)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(texts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m disable_check \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m maybe_is_text(texts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext))\n\u001b[1;32m    221\u001b[0m ):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis does not look like a text document: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Path disable_check to ignore this error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m docname\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages/paperqa/docs.py:248\u001b[0m, in \u001b[0;36mDocs.add_texts\u001b[0;34m(self, texts, doc)\u001b[0m\n\u001b[1;32m    246\u001b[0m     doc\u001b[38;5;241m.\u001b[39mdocname \u001b[38;5;241m=\u001b[39m new_docname\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m texts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(texts):\n\u001b[1;32m    250\u001b[0m         t\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m text_embeddings[i]\n",
      "File \u001b[0;32m/global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages/langchain/embeddings/llamacpp.py:110\u001b[0m, in \u001b[0;36mLlamaCppEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed a list of documents using the Llama model.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, e)) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m embeddings]\n",
      "File \u001b[0;32m/global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages/langchain/embeddings/llamacpp.py:110\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed a list of documents using the Llama model.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, e)) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m embeddings]\n",
      "File \u001b[0;32m/global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages/llama_cpp/llama.py:812\u001b[0m, in \u001b[0;36mLlama.embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    804\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed a string.\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \n\u001b[1;32m    806\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03m        A list of embeddings\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[0;32m/global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages/llama_cpp/llama.py:776\u001b[0m, in \u001b[0;36mLlama.create_embedding\u001b[0;34m(self, input, model)\u001b[0m\n\u001b[1;32m    774\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 776\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)\n\u001b[1;32m    778\u001b[0m total_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n_tokens\n",
      "File \u001b[0;32m/global/common/software/m4388/env/envs/hpc-bootcamp/lib/python3.11/site-packages/llama_cpp/llama.py:471\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_eval returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# Save logits\u001b[39;00m\n\u001b[1;32m    473\u001b[0m rows \u001b[38;5;241m=\u001b[39m n_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mlogits_all \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (8,) into shape (0,)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "docs.add_url('https://www.whitehouse.gov/environmentaljustice/justice40/')\n",
    "end = time.time()\n",
    "print(f'Adding the document took {(end-start):.2f} seconds')\n",
    "\n",
    "answer = docs.query(\"What is Justice40 initiative?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb9182-3359-4ffe-926f-b1ddbbaa9e97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-bootcamp",
   "language": "python",
   "name": "hpc-bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "656fdc43c6a44647078f41991652796517ac498021f62375f4b7c9b76d0f8ed9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
