{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc93fa64",
   "metadata": {},
   "source": [
    "\n",
    "# Web Scraping Tutorial\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Web Scraping](#introduction)\n",
    "2. [Web Scraping Ethics and Legal Considerations](#ethics)\n",
    "3. [Python Libraries for Web Scraping](#libraries)\n",
    "4. [Setting up the Environment](#setup)\n",
    "5. [Basics of HTML](#html)\n",
    "6. [Using Beautiful Soup to Parse HTML](#beautifulsoup)\n",
    "7. [Example Project: Scraping a Simple Webpage](#example)\n",
    "\n",
    "---\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da41eae",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Introduction to Web Scraping <a name=\"introduction\"></a>\n",
    "\n",
    "Web scraping is a method used to extract data from websites. This is done by making a request to the web server for the page's HTML, which is then parsed to extract the desired data. Web scraping is often used for data mining, data analysis, testing, and many other applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a116b",
   "metadata": {},
   "source": [
    "## 2. Web Scraping Ethics and Legal Considerations <a name=\"ethics\"></a>\n",
    "\n",
    "Before starting a web scraping project, it's important to understand the ethical\n",
    "and legal considerations. Not all websites allow web scraping, and even when\n",
    "they allow, they have certain limitations about the rate of scraping. Please\n",
    "also note that data you scraped may be protected by copyright. Always check a website's \"robots.txt\" file and\n",
    "terms of service before scraping.\n",
    "\n",
    "Web scraping is a powerful tool for extracting data from websites. However, it's important to consider the ethical and legal implications before starting a web scraping project. Here is a simple guide to help you navigate these considerations.\n",
    "\n",
    "#### Respect Copyright Laws\n",
    "\n",
    "Web content is often protected by copyright laws. Using this content without permission can lead to legal consequences. Always ensure that the data you are scraping is not copyrighted, or that you have permission to use it.\n",
    "\n",
    "#### Read the Terms of Service\n",
    "\n",
    "Before scraping a website, make sure to read its Terms of Service (ToS). Some websites explicitly forbid web scraping in their ToS. Disregarding this can lead to being banned from the site or even legal action.\n",
    "\n",
    "#### Respect Robots.txt\n",
    "\n",
    "`Robots.txt` is a file used by websites to guide how search engines crawl their pages. It can also provide guidance on which parts of the site the owners prefer not to be scraped. While not legally binding, it's considered good web etiquette to respect the instructions in `robots.txt`.\n",
    "\n",
    "#### Don't Overload the Server\n",
    "\n",
    "Sending too many requests to a website in a short amount of time can overload the server, which can slow down or disrupt service for other users. Be considerate of how many requests you send and try to limit your scraping to off-peak times.\n",
    "\n",
    "#### Be Aware of Privacy Issues\n",
    "\n",
    "Be mindful of privacy issues when scraping personal data. Laws such as the European General Data Protection Regulation (GDPR) or the California Consumer Privacy Act (CCPA) have strict rules about how personal data can be collected and used.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Web scraping is a powerful tool, but it's important to use it responsibly. Always respect the legal rights and privacy of others, and strive to minimize your impact on the services you scrape.\n",
    "\n",
    "Remember, this guide is not legal advice, and laws may vary by location and over\n",
    "time. If you're unsure about the legality of your web scraping project, it's\n",
    "always a good idea to consult with a legal expert.\n",
    "\n",
    "## 3. Python Libraries for Web Scraping <a name=\"libraries\"></a>\n",
    "\n",
    "There are many libraries available in Python to perform web scraping. Some of the most commonly used ones are:\n",
    "\n",
    "- **Requests**: This library allows you to send HTTP requests and handle the response in Python.\n",
    "\n",
    "- **Beautiful Soup**: This library is used for parsing HTML and XML documents. It creates a parse tree from page source code that can be used to extract data in a hierarchical and readable manner.\n",
    "\n",
    "- **Selenium**: Selenium is mainly used for testing in the industry, but it can also be used for web scraping. It provides a way to control a web browser with code, which is useful for interacting with JavaScript-based websites.\n",
    "\n",
    "- **Scrapy**: Scrapy is a powerful and versatile web scraping library, but it can be a bit complex for beginners. It's used for more extensive web scraping projects.\n",
    "\n",
    "In this tutorial, we'll be focusing on using the `requests` and `Beautiful Soup` libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7e8b2",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Setting up the Environment <a name=\"setup\"></a>\n",
    "\n",
    "Before we start coding, we need to install the necessary libraries. This can be done by running the following commands in your Jupyter notebook:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e61fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from requests) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from requests) (2023.5.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/keceli/soft/venvs/energy_justice/lib/python3.11/site-packages (from beautifulsoup4) (2.4.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install requests\n",
    "%pip install beautifulsoup4\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c39ca1",
   "metadata": {},
   "source": [
    "\n",
    "You can import the necessary libraries at the start of your notebook and print\n",
    "their versions. This can help while debugging a problem. \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0044d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "BeautifulSoup version: 4.12.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f'Python version: {sys.version}')\n",
    "import requests\n",
    "import bs4\n",
    "print(f'BeautifulSoup version: {bs4.__version__}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea248fd",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Basics of HTML <a name=\"html\"></a>\n",
    "\n",
    "HTML (HyperText Markup Language) is the standard markup language for creating web pages. It uses tags to define elements, which are the building blocks of web pages. Understanding HTML is essential for web scraping because it allows us to understand the structure of the web page and identify the data we want to extract.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7cd9a6",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Using Beautiful Soup to Parse HTML <a name=\"beautifulsoup\"></a>\n",
    "\n",
    "Beautiful Soup is a Python library that is used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree from page source code that can be used to extract data in a hierarchical and readable manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa64a6",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Example Project: Wikipedia\n",
    "\n",
    "Let's try scraping the Wikipedia page describing high-performance computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce49256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Make a request to the website\n",
    "r = requests.get(\"https://en.wikipedia.org/wiki/High-performance_computing\")\n",
    "r.content\n",
    "\n",
    "# Use the 'html.parser' to parse the page\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "# Extract the text from the page\n",
    "text = soup.get_text()\n",
    "\n",
    "# Extract the URLs from the page\n",
    "urls = []\n",
    "for link in soup.find_all('a'):\n",
    "    urls.append(link.get('href'))\n",
    "\n",
    "# Extract the images from the page\n",
    "images = []\n",
    "for img in soup.find_all('img'):\n",
    "    images.append(img.get('src'))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f92bd3",
   "metadata": {},
   "source": [
    "### Note:\n",
    "While you can access the content by web scraping, it can be hard to parse the required information since we lose the structure embedded in the html format. A better way to access online content is to use custom API that simplifies parsing,i.e. https://github.com/martin-majlis/Wikipedia-API. There are many Python packages that provides custom API for different websites such as https://www.tweepy.org/, https://github.com/Nv7-GitHub/googlesearch. Please, note that all of these tools must be used repsonsibly as desribed in [Web Scraping Ethics and Legal Considerations](#ethics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae35c75-0266-441f-8a12-3089c43b8aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b04364c423c512f1536a27fe426a1c4e4022d4d52be46218bff83eabacc54ea3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
